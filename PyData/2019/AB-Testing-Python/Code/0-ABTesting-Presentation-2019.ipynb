{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"../Resources/Images/Slides-Logo.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "<center> <a href='https://www.linkedin.com/in/raulm8/'>by Raul Maldonado</a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 The Overview\n",
    "\n",
    "## 1.1 Introduction\n",
    "\n",
    "Repo & Details Link\n",
    "\n",
    "http://bit.ly/Raul-ABTesting\n",
    "\n",
    "\n",
    "Data Analyst @Autodesk.\n",
    "Volunteer @DeltaAnalytics and @PyBay\n",
    "\n",
    "Enjoying coffee and running...on the beach.\n",
    "____\n",
    "Today you'll have the opportunity to understand the application of A/B testing in Python and considerations when going through your testing.\n",
    "\n",
    "> Note: Though there exists **proprietary** A/B testing solutions like [Optimizely](https://www.optimizely.com/), [VWO](https://vwo.com/campaign/get-started/?utm_source=google&utm_medium=paid&utm_campaign=mof_search_brand_vwo_brand&utm_content=308583203468&utm_term=vwo&gclid=Cj0KCQjwv8nqBRDGARIsAHfR9wD7uaDnZRUDFKrXDtcn8jCv4v_dNhSRxzWsddKQAo0WuREO4phZ1PQaAopsEALw_wcB), [AB Tasty](https://www.abtasty.com/), [Google Optimize](https://optimize.google.com/optimize/home/), etc, this is another option for your analyses..if you can't pay for said tools. #MoreMoneyMoreProblems\n",
    "\n",
    "Information or data presented today is in no affiliation with Autodesk.\n",
    "\n",
    "\n",
    "[Google Colab version of this Jupyter Noteobok](https://drive.google.com/file/d/1GHMO8xcVkdoi7Q0IYyel4c-eyiZ8K8Ue/view?usp=sharing)\n",
    "\n",
    "Enjoy! :D\n",
    "\n",
    "\n",
    "> Note: If faker is not install, comment and apply the below package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install faker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Concept <span style='color:red'>*</span>\n",
    "\n",
    "A/B Testing is \"[a randomized experiment of two variants, A and B.](https://en.wikipedia.org/wiki/A/B_testing)” This test quantitatively compares two variants/samples with a single \"metric of choice\" in evaluation if there exists a statistical significance between said groups.\n",
    "\n",
    "\n",
    "For example, let's say we ran a digital ad campaign A, with a Call to Action caption 'Click here, please!' \n",
    "\n",
    "Also, let's say we have an alternative ad campaign B with slight modification from A, being the change in the Call to Action to \"Learn more here!\". \n",
    "\n",
    "This in mind, our goal is to decide to see if there is a difference in the campaign's Click Through Rate (CTR) performance such that we increase our engagement.\n",
    "\n",
    "> We define $\\text{CTR} := \\tfrac{\\text{Total Number of Successes}}{\\text{Total Number of Events}}$\n",
    "\n",
    "That is, Considering two ad Campaigns A & B, each with it's unique distinction, we want to evaluate if there is a difference, a statistical difference in fact, in CTR performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Background/History\n",
    "\n",
    "Allen Downey once simplified A/B tests, more generally hypothesis tests, in the following steps:\n",
    "\n",
    "\n",
    "![AB Testing Format](../Resources/Images/ABTesting-Format.png)\n",
    "\n",
    "(Source: [“Probably Overthinking It” by Allen Downey](http://allendowney.blogspot.com/2016/06/there-is-still-only-one-test.html))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BUT what are Hypothesis Tests? These tests essentially the prior/less modern statistical experimental design framework of A/B tests. That being said, some people believe to see that A/B tests were borrowed/taken from the Statistics world, and labeled an edgy/cool name to distinguish iteself.\n",
    "\n",
    "![Not Lame](../Resources/Images/not_lame_meme.png)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 Stats 101?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the above being said, some of you may recall your introduction to Statistics, particularly Hypothesis tests...\n",
    "\n",
    "Generic meme\n",
    "![Maths!](https://media.giphy.com/media/xY4GnaH8P3fc4/source.gif)\n",
    "\n",
    "\n",
    "Topics like the mean $\\mu$ and standard deviation $\\sigma^2$\n",
    "\n",
    "$\\mu = \\sum_{i=1}^N \\tfrac{X_i}{n}$\n",
    "\n",
    "$ \\sigma^2 = \\tfrac{\\sum{(x-\\mu)^2}}{N}$\n",
    "\n",
    "Statistics students are typically introduced to the following methodology and decisions when proceeding in Hypothesis tests. \n",
    "\n",
    "> Similarly, we'll go through this decision tree (\\*slaps knee\\*) to determine what test statistic calculation we'll need for our test.\n",
    "\n",
    "![Tree](../Resources/Images/AB-Testing-Choices-Tree.png)\n",
    "\n",
    "[Source](https://bloomingtontutors.com/blog/when-to-use-the-z-test-versus-t-test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But wait...there's more!\n",
    "\n",
    "The above tree is just one particular approach to determine what test statistic calculation will be a part of your Hypothesis Test. The approach considers the data distribution, data type, and assumption of having a population standard deviation. However, there are other types of questions besides ones related to a t-test or z-test methodology.\n",
    "\n",
    "Considering the question in mind, data & it's type, and more , here are some examples of different tests with respective to assumptions like:\n",
    "\n",
    "![AB-Testing-Choices-Tree](../Resources/Images/Test-Choice-Wiki.png)\n",
    "________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![AB-Testing-Choices-Tree](../Resources/Images/Test-Choice-Current-Scenario.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previously mentioned tree, some of the questions that lead us down that avenue may be:\n",
    "\n",
    "1. I am comparing one sample to an observed case or two samples against one another\n",
    "2. My data types for calculations are similar\n",
    "3. What's the size of my dataset?\n",
    "4. Do I know my standard deviation?\n",
    "\n",
    ">  Other mentions or comparisons between Z-test & T-test, $\\chi^2$ test, and other items from the above are covered in the Appendix section. \n",
    "> But **unfortunately**, we will not cover additional methodologies in this session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 The Format\n",
    "\n",
    "## 2.1 Abstraction\n",
    "\n",
    "Let's disgress from the brief background, and head back to the general overview intially seen in 1.3:\n",
    "\n",
    "![AB Testing Format](../Resources/Images/ABTesting-Format.png)\n",
    "\n",
    "(Source: [“Probably Overthinking It” by Allen Downey](http://allendowney.blogspot.com/2016/06/there-is-still-only-one-test.html))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When implementing the A/B testing procedure in production, we keep in mind of the following steps to ensure a resound test:\n",
    "\n",
    "1. State your Hypothesis \n",
    "2. Statistical Assumptions\n",
    "3. Define and collect control group information\n",
    "4. Identify you Minimum Detectable Effect, Sample Size, and more\n",
    "5. Analyze\n",
    "6. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Testing, Testing..1,2,3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 State your Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Types of Hypothesis Declarations\n",
    "\n",
    "In general, there are 3 types of hypotheses to consider in your tests, seen below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $H_0$ be our Null Hypothesis, a statement saying the variant comparisons have no observed statistical difference \n",
    "\n",
    "Let $H_a$ be our Alternative Hypothesis, and $\\mu$ be the the average of some distribution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Types of Test](../Resources/Images/two_tailed_test.png)\n",
    "\n",
    "[Source](https://www.fromthegenesis.com/difference-between-one-tail-test-and-two-tail-test/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e.g. \n",
    "\n",
    "$H_1: d =\\hat{p_1} - \\hat{p_2} \\leq 0$\n",
    "    \n",
    "    \n",
    "$H_1: d =\\hat{p_1} - \\hat{p_2} \\geq 0$\n",
    "    \n",
    "    \n",
    "$H_1: d =\\hat{p_1} - \\hat{p_2} \\neq 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 The Statement <span style='color:red'>*</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two ad campaigns $A$ and $B$, where $A$ was the pre-existing campaign we've advertised to people.\n",
    "\n",
    "We would like to observe if there is a difference in performance between these groups, particularly, observe if there is a statistically significant difference in their average CTR performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$H_0: d= \\hat{p_1} - \\hat{p_2} = 0$\n",
    "\n",
    "> (i.e. There is no statistical significant difference between the two campaigns)\n",
    "\n",
    "\n",
    "$H_1: d =\\hat{p_1} - \\hat{p_2} \\neq 0$\n",
    "\n",
    "> (i.e. There is a statistical significant difference between the two campaigns)\n",
    "\n",
    "where $\\hat{p} = \\text{CTR}$, defined later in section 3.2.4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We say our **Level of Significance**, $\\alpha$, is set to be $\\alpha = .05$. \n",
    "\n",
    "> This significance level $\\alpha$ is the probability of rejecting the null hypothesis $H_0$, when $H_0$ is true.\n",
    "\n",
    "> I.e. The probability of incorrectly rejecting our original hypothesis when it was actually true, due to chance is 5%.\n",
    "\n",
    "> Lastly, $1-\\alpha$ is our Confidence level, the probability of failing to reject the null hypothesis, when $H_0$ is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 Statement Implications <span style='color:red'>*</span>\n",
    "\n",
    "It should be noted here that we are not trying to prove that there is a significant difference. We are rather observing from the standpoint of no difference to see if one exists.\n",
    "\n",
    "> Imagine being in a state of being leanient until proven guilty, compared to innocent until proven so. The incentive to not be within a neutral standpoint may lead to the illusion of proving or justifying the interest of yourself or another party's agenda, when that's not the case.\n",
    "\n",
    "\n",
    "![Court](../Resources/Images/innocent_court.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.4 4 different decision cases\n",
    "\n",
    "We saw the mention of the significance level $alpha$ and confidence level $1-\\alpha$. However, there are two more situations one can land in their decision.\n",
    "\n",
    "One is the probability of failing to reject the null hypothesis $H_0$, when $H_0$ is False, $\\beta$. \n",
    "\n",
    "That is, what is the probability of sticking to our ways given that there is statistical evidence an alternative scenario exists?\n",
    "\n",
    "And the last probability to mention is $1-\\beta$, which is the probability of rejecting the null hypothesis $H_0$, when $H_0$ is false. This is called the **power** of a test, and is traditionally expected to be .80."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![MDE n'Alpha graph](../Resources/Images/graph_mde_alpha_exp.png)\n",
    "\n",
    "[Source](https://blog.twitter.com/engineering/en_us/a/2016/power-minimal-detectable-effect-and-bucket-size-estimation-in-ab-tests.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![AB Testing Decisions](../Resources/Images/ab_testing_choices.png)\n",
    "\n",
    "[Source](https://www.abtasty.com/blog/type-1-and-type-2-errors/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Statistical Assumptions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Existing Process\n",
    "\n",
    "A user searches for information in Google Search. The user typically will see a list of responses for popular terms or keywords. From their inquiry, depending on what they searched for, they receive an ad at the top of the rankings as a paid recommendation related to their search.\n",
    "\n",
    "\n",
    "### 3.2.2 The Scenario\n",
    "\n",
    "Now, let's say we have an existing Ad Campaign A being run such that the user from above clicks on that ad. Moreover, this campaign A has a particular **Call to Action**, a text to get the user to click said Ad. \n",
    "\n",
    "At the same time, we create or re-activate a new ad Campaign group B, with a different Call to Action.\n",
    "\n",
    "Each ad's activity from a user is logged either as a \"Click\" or an \"No Click\", like so\n",
    "\n",
    "| Date     | Campaign  | User Email       | Action  |\n",
    "|----------|---|---------------------|------------|\n",
    "| 1/1/1900 | B |fake.email@comcast.net   | No Click |\n",
    "| 1/1/1900 | A |real.email@goog1e.com | Click      |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 Experiment Goals \n",
    "\n",
    "~~We want to see if the new campaign will generate us more money.~~ \n",
    "\n",
    "We have an overall goal we obviously want to reach for this test. However, there are too many external, lurking, factors that pose a vunerability to test that out. Moreover, is there a specific metric with respect to our change that can target that effect?\n",
    "\n",
    "Most likely not. We need to focus in on a specific goal respective metric such that we can create a downstream effect to most likely have this overall objective be reached. That being said, in this scenario we want to have users click our ad--that's it. \n",
    "\n",
    "So, if we want to attempt to see a difference in the usability of the two ads A & B, then we would like to look at something like seeing a user being a passive reader, to clicking into the ad.\n",
    "\n",
    "![Clicks](https://media.giphy.com/media/3ogwG8ByATNb5EOm8E/giphy.gif)\n",
    "\n",
    "### 3.2.4 Metric Of Choice <span style='color:red'>*</span>\n",
    "\n",
    "We determine what is our metric of choice for our test, aligned with our existing objectives. Now, we want users to reach out to us, after reviewing our portfolio--seeing an impact for our visibility objective.\n",
    "\n",
    "However, the tracking for that is not set up in our system. Moreover, this ad campaign is an evaluation of the effective usability of the call to action we are implementing. \n",
    "\n",
    "Aligned with our objective to see what ad works for usability between the two variants, we select the Click Through Rate (CTR) as our Metric of Choice.\n",
    "\n",
    "Moreover, let the CTR proportion $\\hat{p}_i$ be defined as $\\hat{p}_i = \\tfrac{\\text{Total Number of Successes}}{\\text{Total Number of Events}} = \\tfrac{x_i}{n_i} $ \n",
    "\n",
    "where \n",
    "\n",
    "$x_i$ are the successes \n",
    "& \n",
    "$n_i$ is the total count of each sample.\n",
    "\n",
    "> Note: those who 'click' an ad is a binary metric, a user either clicks the button at some point, or she/he does not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.5 Basic Assumptions <span style='color:red'>*</span>\n",
    "\n",
    "From the collected samples for this type of targed data/iformation, we assume the following conditions\n",
    "\n",
    "* Each event is independent from one another\n",
    "\n",
    "* Sampling is a simple random sample.\n",
    "\n",
    "* A user either clicks or does not click (an impression) for an ad\n",
    "\n",
    "* Application of the Central Limit Theorem <span style='color:red'>*</span> for normal distribution\n",
    "\n",
    "    * With an appropriate traffic size, the binomial-like distribution of this scenario reaches a Standard Normal (Gaussian) Distribution\n",
    "\n",
    "* Our invariants metrics, metrics not affected by the experiment, are the numbers of users "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.6 Segmentation\n",
    "\n",
    "The type of users we are interested in depends on who our targeted audience is for this testing. \n",
    "(e.g. Do we simply sample form on the overall population of this test, or are we evaluating performance between a particular group? \n",
    "\n",
    "From our previous mention of keywords, Google Ads enables us to selectively target our ads in a bid for certain types of keywords. If we win the bid, our ad is shown related to that keyword. \n",
    "\n",
    "> Note: If our segmentation is too specific, we lead into the impliciation of incorrectly rejecting our existing, null, hypothesis for another--something called the **Simpson's Paradox**. This is because the more refined our segmentation is, we target a specific case or decrease our # of observations such that we lead to those results.\n",
    "\n",
    "> Luckily, to best audit that one can identify if the culmination of different segments have the same results as the total sample "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, identify if segments $A_n$, where $n \\in {1,2,...n}$'s overall results hold true for sample $A$\n",
    "\n",
    "![Simpon's Paradox Example](../Resources/Images/simpons_paradox_example.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy.stats import ttest_ind\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "sns.set_style(\"dark\")\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.7 Minimum Detectable Effect, Sample Size, and Duration\n",
    "\n",
    "#### 3.2.7.1 MDE \n",
    "\n",
    "Assume the original ad's daily CTR performance was, on average, ~50%. That being said, we have a basis for what we know previous to doing anything in the test. \n",
    "> I.e. we have a basis measure to compare this baseline measure of an estimated ~50% CTR \n",
    "\n",
    "Originally, we would like to evaluate if there is a statistical significant difference in the ads performance, under assumption of the baseline measure. \n",
    "\n",
    "And hypothetically say we reject the original assumption $H_0$, such taht we decide to commit to shifting to the new ad campaign, regardless of if it is statistically significantly different. Depending on the nature of the experimentation setup, business logic, costs, and so much more, is this a practical significance in realistically moving to a new implementation? The measuring criteria for this **practical significance** is understanding our **Minimum Dectable Effect (MDE)** for us to consider the new implementation.\n",
    "\n",
    "> An example of MDE is the consideration of cost of investment, change management, or risk.\n",
    "\n",
    "The MDE calculation for our case is defined as \n",
    "\n",
    "$\\text{MDE}:=t^* \\sqrt{\\tfrac{\\hat{p}_1(1-\\hat{p}_1)}{n_1}+\\tfrac{\\hat{p}_1(1-\\hat{p}_2)}{n_2}}$\n",
    "\n",
    "Notice that with the Minimum Detectable Difference in mind, we need to have a few other considerations in our testing:\n",
    "\n",
    "1. Sample Size n\n",
    "    * Duration\n",
    "2. CTR, $\\hat{p}$\n",
    "\n",
    "Though we do not have this measure calculated for us, we can establish what is considered a MDE value for the test. Moreover, we can have a baseline measure $\\hat{p_A}$, from our CTR from campaign A. \n",
    "\n",
    "So, where does that leave the sample size? Given the above, we can still calculate that value + understand the time to reach that # of samples for the campaigns!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.7.2 Sample Size \n",
    "\n",
    "\n",
    "![Samples!](../Resources/Images/costco_samples.jpg)\n",
    "\n",
    "\n",
    "Assuming the two samples have an estimated equal amount of observations & CTR $\\hat{p}_1$ & $\\hat{p}_2$, We can compute variant A's sample size n to reach MDE as\n",
    "\n",
    "\n",
    "$\\text{MDE} =t^*  \\sqrt{\\tfrac{\\hat{p}_1(1-\\hat{p}_1)}{n_1}+\\tfrac{\\hat{p}_2(1-\\hat{p}_2)}{n_2}} \\implies$\n",
    "\n",
    "$ t^*  \\sqrt{\\tfrac{\\hat{p}(1-\\hat{p})}{n}+\\tfrac{\\hat{p}(1-\\hat{p})}{n}} =$\n",
    "\n",
    "$t^*  \\sqrt{2\\tfrac{\\hat{p}(1-\\hat{p})}{n}}$\n",
    "\n",
    "Re-formulating to solve for n, \n",
    "\n",
    "$ \\text{MDE} = t^*  \\sqrt{2\\tfrac{\\hat{p}(1-\\hat{p})}{n}} \\implies $ \n",
    "\n",
    "$(\\tfrac{\\text{MDE}}{t^*})^2   = 2\\tfrac{\\hat{p}(1-\\hat{p})}{n} \\implies $\n",
    "\n",
    "$n= 2 \\hat{p}(1-\\hat{p})(\\tfrac{t^* }{\\text{MDE}})^2$\n",
    "\n",
    "\n",
    "\n",
    "Remember, we have two variants A & B. Therefore, thet total numner of samples neeed is \n",
    "\n",
    "$n = n_1 + n_2 = (2 \\hat{p}(1-\\hat{p})(\\tfrac{t^* }{\\text{MDE}})^2) + (2 \\hat{p}(1-\\hat{p})(\\tfrac{t^* }{\\text{MDE}})^2) $\n",
    "\n",
    "$=4 \\hat{p}(1-\\hat{p})(\\tfrac{t^* }{\\text{MDE}})^2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Example \n",
    "\n",
    "Let ~ t = 2, $\\hat{p} = .51$, and MDE = .05 What would the hypothetical MDE be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example's Estimated MDE is: 1599.0\n"
     ]
    }
   ],
   "source": [
    "t_est = 2 #{Enter value here}\n",
    "p_est =  .51 #{Enter value here}\n",
    "mde = .05 #{Enter value here}\n",
    "\n",
    "# Construct the estimated sample size n_mde, from the formula above \n",
    "n_mde = 4 * p_est * (1-p_est) *  (t_est/0.05)**2 #{Enter formula here}\n",
    "\n",
    "print(f'Example\\'s Estimated MDE is: {round(n_mde, 0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Keep in mind of the following:\n",
    "\n",
    "* Sampling is not linear. So if we want to detect an effect by some other MDE, we have to consider adding a multiple of 4 total observations into data collection.\n",
    "\n",
    "* Tests with smaller sample size can have lower power (I.e.. Tests with lower sample size can only detect large impact)\n",
    "\n",
    "* Also...there are tools out there to calcualted estimated sample size, like Evan Miller's calculator [here](http://www.evanmiller.org/ab-testing/sample-size.html#!51;80;5;5;0)\n",
    "    * E.g. This link will land us to the results for a sample size to be an estimated ~1,600 rows-- Close enough! ;D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 1600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.7.3 Duration\n",
    "\n",
    "For duration, we identify it based on previous traffic history.\n",
    "\n",
    "> As an example, if we needed a combined 20,000 total observations for both groups, and we know we have 4,000 unique users search per week, then we would have to wait 5 weeks for our test to run, assuming traffic flow is at a steady stream\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Data Collection\n",
    "\n",
    "#### 3.3.1 System Setup\n",
    "\n",
    "Assume 3.2.7 assumptions hold true. Moreover, assume that our data collection and experiment would be run fairly from system  setup for the experiment.\n",
    "\n",
    "* In particular,  we are using the Goodle Ads system, and is currently known for being reliable. But how do we know no performance downgrade has occurred (and all hell breaks loose on the internet with a P0 to the next available engineer #HugOps)?\n",
    "\n",
    "#### 3.3.2 Fairness, and A/A Testing\n",
    "\n",
    "To test if our system is set up correctly, either in procedures, allocation of assignments, or other reasons, a tatic used to test if a tool runs experiments fairly is called **A/A testing**. \n",
    "\n",
    "\n",
    "![A, it's A!](../Resources/Images/a_a_test_meme.png)\n",
    "\n",
    "A/A Test tests two identical versions of a page against each other. In an this same-group comparison, the nature of the tool or other factors should show no difference in conversions between the two variant/groups. \n",
    "\n",
    "If there is a statistical significance in this tool, then something is afoot, and your actual implementation increases chances of having incorrect conclusions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.3 Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def campaign_period(startDate,endDate):\n",
    "    from datetime import datetime,timedelta\n",
    "    endDate_dt = datetime.strptime(endDate, '%m-%d-%Y')\n",
    "    startDate_dt = datetime.strptime(startDate,'%m-%d-%Y')\n",
    "    numberOfDays = (endDate_dt - startDate_dt).days\n",
    "    date_list = [endDate_dt - timedelta(days=x) for x in range(numberOfDays)]\n",
    "    date_list.append(startDate_dt)\n",
    "    return(date_list)\n",
    "    #Inspiration via  https://stackoverflow.com/questions/993358/creating-a-range-of-dates-in-python\n",
    "    \n",
    "def campaign_dataset_generator(campaignName = 'PersonDoe-Campaign2019-n', \\\n",
    "                               weightArray=[.5,.5], sample_size = 100, \\\n",
    "                               timeframe = ['7-1-2019', '8-18-2019']):\n",
    "    import random\n",
    "    from faker import Faker\n",
    "    faker = Faker()\n",
    "    actions = ['Click','No Click']\n",
    "    campaignActions = random.choices(actions,\\\n",
    "                        weights=weightArray,\\\n",
    "                        k=sample_size)\n",
    "    campaignTimeFrame = campaign_period(timeframe[0],timeframe[1])\n",
    "    generatedScenario = [[random.choice(campaignTimeFrame), campaignName, faker.email(), i] for i in campaignActions]\n",
    "    return(generatedScenario)\n",
    "\n",
    "def campaign_df_generator(matrix,columns):\n",
    "    campaignsDataset = pd.DataFrame(matrix,\\\n",
    "                                columns = columns)\n",
    "    \n",
    "    campaignsDataset.sort_values(by='Date',\\\n",
    "                             ascending=True, inplace = True)\n",
    "    campaignsDataset = campaignsDataset.pivot_table(index=['Date','Campaign'],\\\n",
    "                                                columns='Action',\\\n",
    "                                                aggfunc='size',\\\n",
    "                                                fill_value=0).reset_index(drop=False)\n",
    "    campaignsDataset[['Click','No Click']] = campaignsDataset[['Click','No Click']].astype(float)\n",
    "\n",
    "    campaignsDataset.rename_axis(None,axis=1,inplace=True)\n",
    "    uniqueCamp =campaignsDataset['Campaign'].unique()\n",
    "    returnedObjects = []\n",
    "    for element in uniqueCamp:\n",
    "        returnedObjects.append(campaignsDataset[campaignsDataset['Campaign'] == element])\n",
    "    return(returnedObjects)\n",
    "\n",
    "def express_campaign_df_generator(campaignList,weightMatrix, sample_size):\n",
    "    firstCamp = campaign_dataset_generator(campaignList[0],weightMatrix[0], sample_size)\n",
    "    secondCamp = campaign_dataset_generator(campaignList[1],weightMatrix[1], sample_size)\n",
    "    combinedCampaigns = [*firstCamp, *secondCamp]\n",
    "    # https://stackoverflow.com/questions/1720421/how-do-i-concatenate-two-lists-in-python\n",
    "    \n",
    "    columns = ['Date','Campaign','User_ID','Action']\n",
    "\n",
    "    dfs = campaign_df_generator(combinedCampaigns, columns)\n",
    "    return(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribution_plot(series, series2=None, name_of_campaign = ''):\n",
    "\n",
    "    sns.distplot( series['CTR'], hist=True, kde=True, \\\n",
    "        kde_kws = {'shade': True, 'linewidth': 3})\n",
    "\n",
    "    plt.title(f'Campaign {name_of_campaign}\\'s Success Distribution')\n",
    "\n",
    "    plt.xlabel(series['CTR'].name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## [Optional] Quick location to change the size for the following datasets\n",
    "# size = 1600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "campaign1_name = 'PersonDoe-Campaign2019-1'\n",
    "campaign2_name = 'PersonDoe-Campaign2019-2'\n",
    "\n",
    "\n",
    "firstCampaign_ds = campaign_dataset_generator(campaignName = campaign1_name, \\\n",
    "                                           weightArray = [.5, .5], \\\n",
    "                                           sample_size = size)\n",
    "\n",
    "secondCampaign_ds = campaign_dataset_generator(campaignName = campaign2_name, \\\n",
    "                                            weightArray = [.53, .47], \\\n",
    "                                            sample_size = size)\n",
    "\n",
    "# Concatenate matrices/arrays\n",
    "\n",
    "combinedCampaigns = [*firstCampaign_ds, *secondCampaign_ds]\n",
    "\n",
    "columns = ['Date','Campaign','User_ID','Action']\n",
    "\n",
    "\n",
    "dfs = campaign_df_generator(combinedCampaigns, columns)\n",
    "\n",
    "campaign1 = dfs[0]\n",
    "campaign2 = dfs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Campaign</th>\n",
       "      <th>Click</th>\n",
       "      <th>No Click</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>PersonDoe-Campaign2019-1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-07-02</td>\n",
       "      <td>PersonDoe-Campaign2019-1</td>\n",
       "      <td>13.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-07-03</td>\n",
       "      <td>PersonDoe-Campaign2019-1</td>\n",
       "      <td>21.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date                  Campaign  Click  No Click\n",
       "0 2019-07-01  PersonDoe-Campaign2019-1   15.0      25.0\n",
       "2 2019-07-02  PersonDoe-Campaign2019-1   13.0       8.0\n",
       "4 2019-07-03  PersonDoe-Campaign2019-1   21.0      17.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out the first 3 columns of dataframe campaign1\n",
    "#{Generate Dataframe with first 3 rows here}\n",
    "campaign1.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Campaign</th>\n",
       "      <th>Click</th>\n",
       "      <th>No Click</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>PersonDoe-Campaign2019-2</td>\n",
       "      <td>19.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-07-02</td>\n",
       "      <td>PersonDoe-Campaign2019-2</td>\n",
       "      <td>20.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2019-07-03</td>\n",
       "      <td>PersonDoe-Campaign2019-2</td>\n",
       "      <td>16.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date                  Campaign  Click  No Click\n",
       "1 2019-07-01  PersonDoe-Campaign2019-2   19.0      17.0\n",
       "3 2019-07-02  PersonDoe-Campaign2019-2   20.0      13.0\n",
       "5 2019-07-03  PersonDoe-Campaign2019-2   16.0      13.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out the first 3 columns of dataframe campaign2\n",
    "#{Generate Dataframe with first 3 rows here}\n",
    "campaign2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Schema:\n",
      "['Date', 'Campaign', 'Click', 'No Click']\n"
     ]
    }
   ],
   "source": [
    "# # Confirm Dataset Schema\n",
    "print(f'DataFrame Schema:\\n{campaign1.columns.tolist()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the dataframe contains dimension information, along with measures like Clicks and No Clicks.\n",
    "\n",
    "However, none of these are not the CTR metric of choice.\n",
    "\n",
    "**Question**\n",
    "\n",
    "Recall the CTR formula from before. Create a column 'CTR' in the both dataframe's campaign1 and campaign2\n",
    "\n",
    "**CTR** $:= \\tfrac{\\text{?}_1}{\\text{?}_2}$\n",
    "\n",
    "Hint: Metric of Choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-13-084c541d2410>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-084c541d2410>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    campaign1['CTR'] = #{Enter CTR formula here}\u001b[0m\n\u001b[0m                                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Calculate the CTR for Dataframe column campaign1['CTR']\n",
    "campaign1['CTR'] = #{Enter CTR formula here}\n",
    "\n",
    "# Calculate the CTR for Dataframe column campaign2['CTR']\n",
    "campaign2['CTR'] = #{Enter CTR formula here}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Daily distribution of the CTR calculations are as followed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Distribution\n",
    "distribution_plot(series = campaign1, name_of_campaign= campaign1_name)\n",
    "\n",
    "# Second Distribution\n",
    "distribution_plot(series = campaign2, name_of_campaign= campaign2_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proportion Calculations\n",
    "\n",
    "n1 = (campaign1['No Click'].sum() + campaign1['Click'].sum())\n",
    "p1 = campaign1['Click'].sum() / n1\n",
    "\n",
    "n2 = (campaign2['No Click'].sum() + campaign2['Click'].sum())\n",
    "p2 = campaign2['Click'].sum()  / n2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Campaign 1: CTR = {p1} and Total Impressions are {n1}')\n",
    "\n",
    "print(f'Campaign 2: CTR = {p2} and Total Impressions are {n2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Analyze The Results <span style='color:red'>*</span>\n",
    "\n",
    "Recall that our test statistic is using the $t$ statistic defined as \n",
    "\n",
    "$t= \\tfrac{ \\hat{p_1} - \\hat{p_2} - 0 }{SE}$, \n",
    "\n",
    "where the standard error $SE = \\sqrt{(\\tfrac{\\hat{p_1} (1-\\hat{p_1})}{n_1}) + (\\tfrac{\\hat{p_2} (1-\\hat{p_2})}{n_2})}$\n",
    "\n",
    "> Note: When comparing between two groups, the notation $d:=\\hat{p_1} - \\hat{p_2}$ is introduced to simplify formula\n",
    "> This is considered a independent t-test with equal variance, and not a dependent t-test w or without equal metrics\n",
    "\n",
    "Moreover, our degrees of freedom for these two variants is defined as $DoF := (n_1 + n_2 -2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1 Manual Approach\n",
    "\n",
    "For the following function, calculate the difference d, standard error, and t-statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ind_t_test(group1,group2, alpha, output_bool=False, state_conclusion =False, express=False):\n",
    "    \n",
    "    # Proportion Calculations.\n",
    "    n1 = (group1['No Click'].sum() + group1['Click'].sum())\n",
    "    p1 = group1['Click'].sum() / float(n1)\n",
    "\n",
    "    n2 = (group2['No Click'].sum() + group2['Click'].sum())\n",
    "    p2 = group2['Click'].sum()  / float(n2)\n",
    "    \n",
    "    d= (p2-p1)\n",
    "    \n",
    "    # Standard Error of Proportions Calculations.\n",
    "    \n",
    "    ## Term 1\n",
    "    ### Calculate the first term p_1 * (1-p_1) /n_1\n",
    "    se1  = # {Create the first term within the SE, in the denominator of t-stats}\n",
    "    \n",
    "    ## Term 2\n",
    "    ### Calculate the first term p_2 * (1-p_2) /n_2\n",
    "    \n",
    "    se2 = # {Create the second term within the SE, in the denominator of t-stats}\n",
    "    \n",
    "    ##### Terms being squared for final result.\n",
    "    ####### sqrt( SE_1 + SE_2)\n",
    "    standardError = # {Enter formula here}\n",
    "    \n",
    "    \n",
    "    # T statistic calculation.\n",
    "    ### d / Standard Error\n",
    "    tStatistic = (d) / standardError # d-0 is not applied here\n",
    "\n",
    "    # Degrees of Freedom\n",
    "    ### (n_1-1) + (n_2-1) = (n_1 + n_2 -2)\n",
    "    dof = (n1 + n2-2)\n",
    "    \n",
    "    \n",
    "    # Critical T Value Test Statistic\n",
    "    criticalValue = scipy.stats.t.ppf(1.0 - alpha, df = dof)\n",
    "\n",
    "\n",
    "    # Confidence Interval\n",
    "    ### Avoid value of 0 in this CI\n",
    "    confInt = [d - (criticalValue * standardError), d + (criticalValue * standardError)]\n",
    "\n",
    "    \n",
    "    # Second type of decision criteria: |t| >= |t^*|.\n",
    "\n",
    "    # Two Sided P Value, P( |t| >= |t^*|).\n",
    "    ### Calcualte the p-value using a Cumulative Density function\n",
    "    ### from Scipy's stats.t.cdf(t-test, DoF) function\n",
    "    \n",
    "    p_val = #{enter calculation w/function from above, here}\n",
    "    \n",
    "    if output_bool is True:\n",
    "        print('Analysis:\\n\\n')\n",
    "        print(f'Campaign {group1.Campaign[0]}\\'s CTR: {round(p1,4)}' \\\n",
    "              + f' with Standard Error {se1}.')\n",
    "        print(f'Campaign {group2.Campaign[3]}\\'s CTR: {round(p2,4)}' \\\n",
    "              + f' with Standard Error {se2}.\\n')\n",
    "        \n",
    "        print(f\"Confidence Interval {confInt}\")\n",
    "        print(f'T Statistic: {round(tStatistic, 2)}\\n')\n",
    "        \n",
    "        print(f'We have critical value t^* at {round(criticalValue, 2)}' + \\\n",
    "              f'\\nand p-value of {round(p_val, 2)}')\n",
    "        \n",
    "        print(f'\\n\\nComponents for variants Campaign {group1.Campaign[0]}\\'s \\n& ' + \\\n",
    "              f'Campaign {group2.Campaign[3]}\\'s, respectively:')\n",
    "        \n",
    "        print(f'Difference d: {d}')\n",
    "        \n",
    "        print(f'SE terms within SE calculation: {[se1,se2]}')\n",
    "        print(f'SE: {standardError}')\n",
    "        \n",
    "        print(f'Calcualted T-statistic: {tStatistic}')\n",
    "        print(f'T critical value: {criticalValue}')\n",
    "        \n",
    "    if state_conclusion is True:\n",
    "        if express is False:\n",
    "            # Restate our decision process\n",
    "            print('Conclusion:\\n\\n')\n",
    "\n",
    "            print(f'If the p-value is less than our defined alpha = {alpha}, then we' +\\\n",
    "                  ' reject the null hypothesis H_0.\\nIf not, then we fail to reject the' +\\\n",
    "                  ' null hypothesis H_0.')\n",
    "\n",
    "            print(f'Confidence Interval: {confInt}')\n",
    "            print(f'P-value: {p_val}')\n",
    "            print(f'Alpha: {alpha}')\n",
    "        \n",
    "        if p_val < alpha:\n",
    "            print('\\nWe reject the Null Hypothesis H_0')\n",
    "            print('Therefore, we can say that there is a statistical ' + \\\n",
    "            'difference between the two campaigns.')\n",
    "\n",
    "        else:\n",
    "            print('\\nWe fail to reject the Null Hypothesis H_0')\n",
    "            print('\\nTherefore, we can say that there is no statistical' + \\\n",
    "            ' significant difference between the two campaigns.')\n",
    "    return([p1,p2], [n1,n2], \\\n",
    "           [se1,se2], standardError, \\\n",
    "           tStatistic, criticalValue, \\\n",
    "           confInt, p_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the calculated t-statistic $t$ > critical value $t^*$, then we reject the null hypothesis $H_0$, and accept the alternative hypothesis $H_1$.\n",
    "\n",
    "\n",
    "> Equivalently, if $p < \\alpha$, then we  reject the null hypothesis $H_0$. \n",
    "\n",
    ">> P-value is the probability of obtaining an effect at least as extreme as the one in your sample data, assuming null hypothesis is true.\n",
    "\n",
    "It does not measure support for alternative hypothesis\n",
    "\n",
    "If not, then we fail to reject the null hypothesis $H_0$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Sleepy](../Resources/Images/tired_gif.gif)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctr, samples, \\\n",
    "sample_se, SE, \\\n",
    "tStat, tCrit, \\\n",
    "confidence_interval, p_val = ind_t_test(group1 = campaign1,\n",
    "                                        group2= campaign2, \\\n",
    "                                        alpha = alpha,\\\n",
    "                                        output_bool=True, \\\n",
    "                                        state_conclusion=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2 Scipy Approach\n",
    "\n",
    "We use Scipy's [Independent T-Test](https://github.com/scipy/scipy/blob/v0.14.0/scipy/stats/stats.py#L3114) with with an assumed equal variance between both samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try out the Independent T-Test w/Equal Variance function from Scipy's \n",
    "# stats.ttest_ind(group 1, group 2, equal_var = ?)\n",
    "### Remember, groups are of series campaign2['CTR'] in Dataframe campaign2\n",
    "\n",
    "\n",
    "tTest_statistic, tTest_pval = scipy.stats.ttest_ind(campaign2['CTR'], \\\n",
    "                                                    campaign1['CTR'], \\\n",
    "                                                    equal_var = True)\n",
    "\n",
    "print(f'Scipy\\'s Calculated t-statistic is {round(tTest_statistic,2)}' + \\\n",
    "      f' and p-value is {tTest_pval}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it, that's the test.\n",
    "\n",
    "![Awk](../Resources/Images/awkward_meme.png)\n",
    "\n",
    "Oh, but we should still respectively state our conclusion (for respect of the test)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tTest_pval < alpha:\n",
    "    print('\\nWe reject the Null Hypothesis H_0')\n",
    "    print('Therefore, we can say that there is a statistical ' + \\\n",
    "    'difference between the two campaigns.')\n",
    "\n",
    "else:\n",
    "    print('\\nWe fail to reject the Null Hypothesis H_0')\n",
    "    print('\\nTherefore, we can say that there is no statistical' + \\\n",
    "    ' significant difference between the two campaigns.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, notice the slight differences between the manual approach and the Scipy approach.\n",
    "There may be some error in setup, rounding, or another type of error in the Manual approach, most likely!\n",
    "\n",
    "With that being said, remember...\n",
    "\n",
    "\n",
    "![Always be Testing](../Resources/Images/always_be_testing.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or to others...\n",
    "\n",
    "![A/B testing, here we go again](../Resources/Images/here_we_go_again.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0 Remarks and Considerations <span style='color:red'>*</span>\n",
    "\n",
    "[“Ignorance more frequently begets confidence than does knowledge”](https://www.goodreads.com/quotes/24141-ignorance-more-frequently-begets-confidence-than-does-knowledge-it-is)\n",
    "\n",
    "― Charles Darwin, The Descent of Man"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 P-Hacking\n",
    "\n",
    "* Be weary on being incentivized to prove something is different\n",
    "    * Don't shape results, after the fact\n",
    "* During the test\n",
    "    * Don't peek\n",
    "        * Avoid t-test timelines, though some companies may have that option\n",
    "    * Refrain from stopping test at point of first statistical difference result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Sample Size Matters\n",
    "\n",
    "Here’s how small effect sizes can still produce tiny p-values:\n",
    "\n",
    "You have a very large sample size. As the sample size increases, the hypothesis test gains greater statistical power to detect small effects. With a large enough sample size, the hypothesis test can detect an effect that is so miniscule that it is meaningless in a practical sense.\n",
    "\n",
    "The sample variability is very low. When your sample data have low variability, hypothesis tests can produce more precise estimates of the population’s effect. This precision allows the test to detect tiny effects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Interpreting the probability $\\alpha$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo on Alpha value\n",
    "\n",
    "for i in range(10):\n",
    "    expressCamp1, expressCamp2 = express_campaign_df_generator([campaign1_name,campaign2_name], \\\n",
    "                                      weightMatrix = [[.5, .5],[.53, .47]],\\\n",
    "                                      sample_size = 3000)\n",
    "    ind_t_test(group1 = expressCamp1, \\\n",
    "               group2 = expressCamp2, \\\n",
    "               alpha=0.05, \\\n",
    "               output_bool=False, \\\n",
    "               state_conclusion=True, \\\n",
    "              express = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Practical vs Significance\n",
    "\n",
    "Statistical significance indicates only that you have sufficient evidence to conclude that an effect exists. It is a mathematical definition that does not know anything about the subject area and what constitutes an important effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Central Limit Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Central Limit Theorem (CLT) implies a sample of independent random variables, their sums tends towards to a normal distribution even if the original variables themselves aren't normally distributed, also the sample mean tends towards to a normal distribution (sum and mean are equivalent).\n",
    "\n",
    "\n",
    "That is, For large values of n, the distributions of the count $X$ and the sample proportion are approximately normal due to the Central Limit Theorem, as it approximates the normal distribution like \n",
    "\n",
    "$\\bar{X}$ ~ $N(\\tfrac{np}{p},\\tfrac{np(1-p)}{n^2}) = N(n, \\tfrac{p(1-p)}{n})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 More on timing\n",
    "\n",
    "* When is the best time to run the experiment? (e.g. running on Holidays, weekends, weekdays, etc?)\n",
    "* If there is a new change, there may be a learning effect, and then behaviour can become stable again (leading to a false positive)\n",
    "    * Recommendation to avoid a Learning Effect is running experiment on smaller group of users for an extended time period.\n",
    "    \n",
    "## 4.7 More on Simpson's Paradox\n",
    " \n",
    " \n",
    "* Mind the possibility of the Simpson's Paradox\n",
    "    * Segmentation of groups in thus reducing sampling creates significance, but not from other groups from data before segmentation. Best way to validate this doesn't exist is to do the same test for combined groups.\n",
    "        * E.g. Splitting between new users and existing users\n",
    "* $\\alpha$ percent of the time, you'll reach significance due to chance \n",
    "    * You are running a tests with 20 variants, and you test each hypothesis separately:\n",
    "        * P(one significant result) = 1−P(no significant results) \n",
    "        * P(one significant result) = 1−(1−0.05)^20 = 0.64\n",
    "    * Avoid this by the Bonferroni Correction\n",
    "\n",
    "## 4.8 On the note of the Decision Tree image...\n",
    "\n",
    "Differences between z & t tests:\n",
    "\n",
    "* z-test: A z-test assumes observations are independently drawn from Normal Distribution with unknown mean and known variance. Z-test is used when we have quantitative data.\n",
    "\n",
    "* t-test: a t-test assumes observations are independently drawn from Normal distribution with unknown mean and unknown variance. With a t-test, we do not know the population variance.\n",
    "\n",
    "## 4.9 Other\n",
    "\n",
    "* Ethics\n",
    "    * Do your users know about being tested on?\n",
    "    * Privacy concerns \n",
    "    * Post-testing effects on participants\n",
    "        * IRB (Institutional Review Board) is not necessary unless farmful actions happen to test participants\n",
    "        * Formal and regulated tests require knowledge of privacy, choice, risk, and informed consent\n",
    "    \n",
    "    \n",
    "* Novelty Effect or Change Aversion: Post-test Cohort analysis may be helpful in evaluation if testing was valid for users, after some timeframe after the test.\n",
    "\n",
    "### 4.9.1 Other: Welche's T-test (T test with unequal variance or observations)\n",
    "\n",
    "Equal or unequal sample sizes, unequal variances\n",
    "This test, also known as Welch's t-test, is used only when two population variances are not assumed to be equal (the two sample sizes may or may not be equal) and hence must be estimated seperately. The t-stat to test whether population means are different is\n",
    "\n",
    "$t= \\tfrac{\\bar{X}_1 - \\bar{X}_2}{s_{\\bar{\\Delta}}}$\n",
    "\n",
    "where $s_{\\bar{\\Delta}} = \\sqrt{\\tfrac{s^2_1}{n_1} + \\tfrac{s^2_2}{n_2}}$\n",
    "\n",
    "d.f. = $\\tfrac{(\\tfrac{s^2_1}{n_1} + \\tfrac{s^2_2}{n_2})^2}{\\tfrac{(\\tfrac{s^2_1}{n_1})^2}{n_1 - 1} + \\tfrac{(\\tfrac{s^2_2}{n_2})^2}{n_2 - 1}}$\n",
    "\n",
    "This d.f. is known as the Welch-Satterthwaite equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='appendix-chi-square-test'></a>\n",
    "\n",
    "\n",
    "\n",
    "# 5.0 The Chai-err umm....Chi-Squared Test! [Optional]\n",
    "\n",
    "\n",
    "## 5.1 $\\chi^2$ Testing [Optional]\n",
    "\n",
    "\n",
    "* Chi-Square Goodness of Fit Test\n",
    "    * $\\chi^2$ Test that determines if a sample data matches a population. For more details on this type, see: Goodness of Fit Test.\n",
    "\n",
    "* Chi-Square Test for Independence\n",
    "\n",
    "    * $\\chi^2$ Test that compares two variables in a contingency table to see if they are related. In a more general sense, it tests to see whether distributions of categorical variables differ from each another.\n",
    "    \n",
    "> Note: Chi Square is cool because it works with more than 2 samples\n",
    "\n",
    "> Note: If we have a  small sample size, then Chi Square may have more errors, and thus one would have to work with a Fischer's Exact Test\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Applying the $\\chi^2$ Test \n",
    "\n",
    "### 5.2.1 The Manual Approach\n",
    "Equivalently, we can use a similiar process and test statistic in evaluation of proportions, seen in section 3.0+. \n",
    "\n",
    "Let us have a 2x2 frequency table where columns are two groups of respondents and rows are the two responses \"Clicks\" (our successes) and \"No Clicks\". \n",
    "\n",
    "\n",
    "|    _    | Click | No Click | \n",
    "|--------|--------|-------------|\n",
    "| Test 1 | X_1    | Y_1         |\n",
    "| Test 2 | X_2    | Y_2         |\n",
    "| Total  | x      | x           | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State the Hypothesis:\n",
    "\n",
    "$H_0: \\bar{p_1} - \\bar{p_2} = 0$ \n",
    "\n",
    "$H_1: \\bar{p_1} - \\bar{p_2} \\neq 0$\n",
    "\n",
    "Degrees of freedom = $(x_1 - 1) * (x_2 - 1)$\n",
    "\n",
    "Test Statistic:\n",
    "\n",
    "$\\chi^2= \\tfrac{(O-E)^2}{E}$, \n",
    "\n",
    "where $O$ are the Observed values and $E$ are the Expected values.\n",
    "\n",
    "> Note:\n",
    "Expected values are calculated as such:\n",
    "> For the top left region, it would be $\\tfrac{( x_{Clicks} * (X_1 + Y_1)}{x_T }$\n",
    "\n",
    "|    _    | Clicks | No Clicks | Total (Impressions) |\n",
    "|--------|--------|-------------|-----|\n",
    "| Test 1 | $Y_1$    | $Y_1$         | $X_1 + Y_1$  | \n",
    "| Test 2 | $X_2 $   | $Y_2 $        | $X_2 + Y_2 $  |\n",
    "| Total  | $x_{Clicks}$      | $x_{No Clicks}$        | $x_T$   | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = [ [campaign1['Click'].sum(), \\\n",
    "        campaign1['No Click'].sum(),\\\n",
    "        campaign1['No Click'].sum()+campaign1['Click'].sum()], \\\n",
    "      [campaign2['Click'].sum(), \\\n",
    "       campaign2['No Click'].sum(), \\\n",
    "        campaign2['No Click'].sum()+campaign2['Click'].sum()] ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twoByTwo_df = pd.DataFrame(df,index=['Campaign 1', 'Campaign 2'] ,\\\n",
    "                           columns=['Click','No Click','Impressions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twoByTwo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expectedClicksSeries = (twoByTwo_df['Impressions']/twoByTwo_df['Impressions'].sum()) * twoByTwo_df['Click'].sum() \n",
    "\n",
    "expectedNonClicksSeries = (twoByTwo_df['Impressions']/twoByTwo_df['Impressions'].sum()) * twoByTwo_df['No Click'].sum() \n",
    "\n",
    "expectedDf = pd.concat([expectedClicksSeries,expectedNonClicksSeries],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expectedDf.columns =['Click', 'No Click']\n",
    "expectedDf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chiSquareStatistic = ((twoByTwo_df[['Click','No Click']]-expectedDf)**2 / expectedDf).values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chiSquareStatistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2 The SciPy Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chiSquareStat, pVal, dof, expected = scipy.stats.chi2_contingency(twoByTwo_df)\n",
    "\n",
    "print('Expected Table: \\n',expected,'\\n')\n",
    "\n",
    "# interpret test-statistic\n",
    "prob = 0.95\n",
    "alpha = 1.0 - prob\n",
    "\n",
    "criticalVal = scipy.stats.chi2.ppf(prob, dof)\n",
    "\n",
    "print(f'Degrees of Freedom: {dof}\\n')\n",
    "print('probability=%.2f, critical=%.2f, stat=%.2f \\n' % (prob, criticalVal, alpha))\n",
    "\n",
    "print('Decision:')\n",
    "print(f'For significance level {round(alpha,2)},\\n')\n",
    "\n",
    "if abs(chiSquareStat) >= criticalVal:\n",
    "    print('We reject the Null Hypothesis, H_0\\n')\n",
    "    print(f'for p = {prob} >= {criticalVal}.')\n",
    "else:\n",
    "    print('We fail to reject the Null Hypothesis, H_0\\n')\n",
    "    print(f'for p = {prob} < {criticalVal}.')\n",
    "\n",
    "    \n",
    "### Alternatively can say:    \n",
    "# if p <= alpha:\n",
    "#     print('We reject the Null Hypothesis, H_0.')\n",
    "#     print(f'for p = {p} >= {alpha}.')\n",
    "\n",
    "# else:\n",
    "#     print('We fail to reject the Null Hypothesis, H_0.')\n",
    "#     print(f'for p = {p} < {alpha}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.0 Resources\n",
    "\n",
    "* https://byrony.github.io/understanding-ab-testing-and-statistics-behind.html\n",
    "    \n",
    "* https://www.dataquest.io/blog/a-b-testing-the-definitive-guide-to-improving-your-product/\n",
    "\n",
    "* https://docs.google.com/presentation/d/1k_zR5IkHaIpA6fbTDoa48Gh_LodY7Mjbf56zLro5Uus/edit#slide=id.g31d3c8507d_0_177\n",
    "        \n",
    "        \n",
    "* https://stats.stackexchange.com/questions/76875/what-is-the-difference-between-mcnemars-test-and-the-chi-squared-test-and-how/141450#141450\n",
    "\n",
    "* https://github.com/scipy/scipy/blob/v0.14.0/scipy/stats/stats.py#L3114\n",
    "\n",
    "* https://help.optimizely.com/Analyze_Results/How_long_to_run_an_experiment#Baseline_Conversion_Rate\n",
    "\n",
    "* [“Ignorance more frequently begets confidence than does knowledge”](https://www.goodreads.com/quotes/24141-ignorance-more-frequently-begets-confidence-than-does-knowledge-it-is)\n",
    "\n",
    "* https://blog.twitter.com/engineering/en_us/a/2016/power-minimal-detectable-effect-and-bucket-size-estimation-in-ab-tests.html\n",
    "\n",
    "* https://stackoverflow.com/questions/1720421/how-do-i-concatenate-two-lists-in-python\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (DataScienceKernel)",
   "language": "python",
   "name": "datasciencekernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
